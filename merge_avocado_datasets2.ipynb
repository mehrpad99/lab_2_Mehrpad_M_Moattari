{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxKWdoysyHJ"
      },
      "source": [
        "**Notebook description:**\n",
        "\n",
        "This notebook can be used to update [this][1] dataset by merging it with the new data downloaded from [the Hass Avocado Board website][2].\n",
        "\n",
        "[1]: https://www.kaggle.com/timmate/avocado-prices-2020\n",
        "[2]: https://hassavocadoboard.com/category-data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOM0HvpvJmi"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UldOUIWQu0dV"
      },
      "source": [
        "# Put csv files with the new data into that directory.\n",
        "DATASETS_DIR = 'new_avocado_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH4QBfICsyHa"
      },
      "source": [
        "## Read in the original (updated upto 2020, not Justin's) dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7xHKXK1OsyHc"
      },
      "source": [
        "original_dataset_path = 'avocado-updated-2020.csv'\n",
        "original_df = pd.read_csv(original_dataset_path,\n",
        "                          parse_dates=['date'],\n",
        "                          index_col='date')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYCtqR9GsyH4"
      },
      "source": [
        "## Read in the new data and merge it with the original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LDWhoVgvsyIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "outputId": "c09e069a-c1cb-411c-b8ef-11d876efd85e"
      },
      "source": [
        "import os\n",
        "\n",
        "# Define a dictionary for renaming names of columns of the new datasets.\n",
        "RENAMED_COLS_DICT = {\n",
        "    'asp_current_year': 'average_price',\n",
        "    'total_bulk_and_bags_units': 'total_volume',\n",
        "    '4046_units': '4046',\n",
        "    '4225_units': '4225',\n",
        "    '4770_units': '4770',\n",
        "    'totalbagged_units': 'total_bags',\n",
        "    'smlbagged_units': 'small_bags',\n",
        "    'lrgbagged_units': 'large_bags',\n",
        "    'x-lrgbagged_units': 'xlarge_bags'\n",
        "}\n",
        "\n",
        "cat_df = original_df.copy()  # a df for concatenating (i.e., merging) the data\n",
        "\n",
        "print(\"original dataset's shape:\",  original_df.shape)\n",
        "print()\n",
        "\n",
        "# Define an accumulatator of the number of entries. This accumulator is used\n",
        "# for the testing purposes at the bottom of this cell.\n",
        "n_total_entries_accumulator =  original_df.shape[0]\n",
        "\n",
        "filenames = sorted(os.listdir(DATASETS_DIR))\n",
        "\n",
        "for filename in filenames:\n",
        "    base_filename, extension = filename.split('.')\n",
        "\n",
        "    if extension != 'csv':\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        print('processing', filename, '...')\n",
        "\n",
        "        file_path = os.path.join(DATASETS_DIR, filename)\n",
        "        tmp_df = pd.read_csv(file_path,\n",
        "                             parse_dates=['Current Year Week Ending'],\n",
        "                             index_col='Current Year Week Ending')\n",
        "\n",
        "        print(\"dataset's shape:\", tmp_df.shape)\n",
        "\n",
        "        tmp_df.drop('Timeframe', axis=1, inplace=True)\n",
        "        tmp_df.sort_values(['Current Year Week Ending', 'Geography'], inplace=True)\n",
        "\n",
        "        # Lower the columns' names and replace spaces with underscores.\n",
        "        tmp_df.rename(lambda col_name: col_name.lower().replace(' ', '_'), axis=1, inplace=True)\n",
        "        tmp_df.rename(RENAMED_COLS_DICT, axis=1, inplace=True)\n",
        "        tmp_df.index.name = 'date'\n",
        "        tmp_df.type.replace('Conventional ', 'Conventional', inplace=True)\n",
        "        assert tmp_df.type.nunique() == 2, 'dataset` must contain only 2 types of avocados'\n",
        "\n",
        "        tmp_df.type = tmp_df.type.apply(lambda avocado_type: avocado_type.lower())\n",
        "        tmp_df['year'] = tmp_df.index.year\n",
        "\n",
        "        print('adding', tmp_df.shape[0], 'entries to the original dataset...')\n",
        "        print()\n",
        "\n",
        "        cat_df = pd.concat([cat_df, tmp_df], axis=0)\n",
        "        n_total_entries_accumulator += tmp_df.shape[0]\n",
        "\n",
        "print(\"final merged dataset's shape:\", cat_df.shape)\n",
        "print('number of entries in the merged dataset should be:', n_total_entries_accumulator)\n",
        "\n",
        "assert cat_df.geography.nunique() == 54, 'merged dataset must have 54 unique geographical names'\n",
        "\n",
        "# Drop all duplicates (if any).\n",
        "print()\n",
        "print('dropping duplicates...')\n",
        "\n",
        "n_entries_before = cat_df.shape[0]\n",
        "cat_df.drop_duplicates(inplace=True)\n",
        "n_entries_after = cat_df.shape[0]\n",
        "n_entries_dropped = n_entries_before - n_entries_after\n",
        "\n",
        "print(f'dropped {n_entries_dropped} duplicates.')\n",
        "print(\"merged dataset's shape after dropping duplicates:\", cat_df.shape)\n",
        "\n",
        "# Sort the merged dataset just in case (not really needed as the data should be\n",
        "# already sorted).\n",
        "cat_df.sort_values(['date', 'geography'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original dataset's shape: (30021, 12)\n",
            "\n",
            "processing 2020-plu-total-hab-data.csv ...\n",
            "dataset's shape: (2160, 12)\n",
            "adding 2160 entries to the original dataset...\n",
            "\n",
            "final merged dataset's shape: (32181, 12)\n",
            "number of entries in the merged dataset should be: 32181\n",
            "\n",
            "dropping duplicates...\n",
            "dropped 2160 duplicates.\n",
            "merged dataset's shape after dropping duplicates: (30021, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfC_HPTsyIK"
      },
      "source": [
        "## Save the updated (merged) dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EidA0aL4syIM"
      },
      "source": [
        "# Save with index as it contains the dates.\n",
        "cat_df.to_csv('avocado-updated-again-2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}