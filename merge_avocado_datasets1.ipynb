{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMxKWdoysyHJ"
      },
      "source": [
        "**Notebook description:**\n",
        "\n",
        "This notebook was used to create [the updated version][1] of [the original avocado dataset][2] by merging the data from the original dataset and the new data for 2018 -- 2020 downloaded from [the Hass Avocado Board website][3]. For more information on how the updated dataset differs from the original one, please refer to [this][1] page.\n",
        "\n",
        "[1]: https://www.kaggle.com/timmate/avocado-prices-2020\n",
        "[2]: https://www.kaggle.com/neuromusic/avocado-prices\n",
        "[3]: https://hassavocadoboard.com/category-data/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFOM0HvpvJmi"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UldOUIWQu0dV"
      },
      "source": [
        "# Put csv files with the new data in that directory.\n",
        "DATASETS_DIR = 'new_avocado_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH4QBfICsyHa"
      },
      "source": [
        "## Read in and preprocess Justin's dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7xHKXK1OsyHc"
      },
      "source": [
        "original_dataset_path = 'avocado.csv'\n",
        "original_df = pd.read_csv(original_dataset_path,\n",
        "                          parse_dates=['Date'],\n",
        "                          index_col='Date')\n",
        "\n",
        "original_df.sort_values(['Date', 'region'], inplace=True)\n",
        "original_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
        "\n",
        "# Lower the columns' names and replace spaces with underscores.\n",
        "original_df.rename(lambda col_name: col_name.lower().replace(' ', '_'),\n",
        "                   axis=1, inplace=True)\n",
        "\n",
        "original_df.rename({'averageprice': 'average_price', 'region': 'geography'},\n",
        "                   axis=1, inplace=True)\n",
        "\n",
        "# Remove the existing data from 2018 as there is just little of data for that\n",
        "# year, hence, we have to add the 2018 data from the Hass Avocado Board CSV\n",
        "# file. If we do not remove the existing entries for the 2018, we will get\n",
        "# duplicates after merging the original dataset with the complete 2018 data.\n",
        "original_df = original_df.query('year != 2018')\n",
        "original_df.index.name = 'date'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbKalNlUsyHn"
      },
      "source": [
        "Rename values in the `region` column so that they match the names from the original Hass Avocado Board data, e.g. `SanFrancisco` will be renamed to `San Francisco` and `BaltimoreWashington` will be renamed to `Baltimore/Washington`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "puiVhm-ksyHp"
      },
      "source": [
        "import os\n",
        "\n",
        "# Read one of the datasets with the original geographical names of places.\n",
        "tmp_df = pd.read_csv(os.path.join(DATASETS_DIR, '2018-plu-total-hab-data.csv'))\n",
        "\n",
        "# The number of geographical names should be the same in both datasets and\n",
        "# should equal to 54.\n",
        "assert original_df.geography.nunique() == tmp_df.Geography.nunique(), \\\n",
        "    'number of geographical names is not the same'\n",
        "\n",
        "# Create a dictionary for renaming the geographical names in the original\n",
        "# dataset.\n",
        "renamed_regions_dict = {}\n",
        "n_regions = tmp_df.Geography.nunique()  # should be 54\n",
        "correct_region_names = sorted(tmp_df.Geography.unique())\n",
        "incorrect_region_names = sorted(original_df.geography.unique())\n",
        "\n",
        "for i in range(n_regions):\n",
        "    correct_region_name = correct_region_names[i]\n",
        "    incorrect_region_name = incorrect_region_names[i]\n",
        "\n",
        "    if correct_region_name == incorrect_region_name:\n",
        "        # If the correct name is the same as the incorrect one, just skip.\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        # Add a pair like 'SanFrancisco - San Francisco' to the renaming dict.\n",
        "        renamed_regions_dict[incorrect_region_name] = correct_region_name\n",
        "\n",
        "# Rename the geographical names in the original dataset.\n",
        "original_df.geography.replace(renamed_regions_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYCtqR9GsyH4"
      },
      "source": [
        "## Read in the new data for 2018 -- 2020 and merge it with  Justin's dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LDWhoVgvsyIA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "cee9bab7-371d-452b-8fdc-8de89492c720"
      },
      "source": [
        "# Define a dictionary for renaming names of columns of the new datasets.\n",
        "RENAMED_COLS_DICT = {\n",
        "    'asp_current_year': 'average_price',\n",
        "    'total_bulk_and_bags_units': 'total_volume',\n",
        "    '4046_units': '4046',\n",
        "    '4225_units': '4225',\n",
        "    '4770_units': '4770',\n",
        "    'totalbagged_units': 'total_bags',\n",
        "    'smlbagged_units': 'small_bags',\n",
        "    'lrgbagged_units': 'large_bags',\n",
        "    'x-lrgbagged_units': 'xlarge_bags'\n",
        "}\n",
        "\n",
        "cat_df = original_df.copy()  # a df for concatenating (i.e., merging) the data\n",
        "\n",
        "print(\"original dataset's shape:\",  original_df.shape)\n",
        "print()\n",
        "\n",
        "# Define an accumulatator of the number of entries. This accumulator is used\n",
        "# for the testing purposes at the bottom of this cell.\n",
        "n_total_entries_accumulator =  original_df.shape[0]\n",
        "\n",
        "filenames = sorted(os.listdir(DATASETS_DIR))\n",
        "\n",
        "for filename in filenames:\n",
        "    base_filename, extension = filename.split('.')\n",
        "\n",
        "    if extension != 'csv':\n",
        "        continue\n",
        "\n",
        "    else:\n",
        "        print('processing', filename, '...')\n",
        "\n",
        "        file_path = os.path.join(DATASETS_DIR, filename)\n",
        "        tmp_df = pd.read_csv(file_path,\n",
        "                             parse_dates=['Current Year Week Ending'],\n",
        "                             index_col='Current Year Week Ending')\n",
        "\n",
        "        print(\"dataset's shape:\", tmp_df.shape)\n",
        "\n",
        "        tmp_df.drop('Timeframe', axis=1, inplace=True)\n",
        "        tmp_df.sort_values(['Current Year Week Ending', 'Geography'], inplace=True)\n",
        "\n",
        "        # Lower the columns' names and replace spaces with underscores.\n",
        "        tmp_df.rename(lambda col_name: col_name.lower().replace(' ', '_'), axis=1, inplace=True)\n",
        "        tmp_df.rename(RENAMED_COLS_DICT, axis=1, inplace=True)\n",
        "        tmp_df.index.name = 'date'\n",
        "        tmp_df.type.replace('Conventional ', 'Conventional', inplace=True)\n",
        "        assert tmp_df.type.nunique() == 2, 'dataset` must contain only 2 types of avocados'\n",
        "\n",
        "        tmp_df.type = tmp_df.type.apply(lambda avocado_type: avocado_type.lower())\n",
        "        tmp_df['year'] = tmp_df.index.year\n",
        "\n",
        "        print('adding', tmp_df.shape[0], 'entries to the original dataset...')\n",
        "        print()\n",
        "\n",
        "        cat_df = pd.concat([cat_df, tmp_df], axis=0)\n",
        "        n_total_entries_accumulator += tmp_df.shape[0]\n",
        "\n",
        "print(\"final merged dataset's shape:\", cat_df.shape)\n",
        "print('number of entries in the merged dataset should be:', n_total_entries_accumulator)\n",
        "\n",
        "assert cat_df.geography.nunique() == 54, 'merged dataset must have 54 unique geographical names'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original dataset's shape: (16953, 12)\n",
            "\n",
            "processing 2018-plu-total-hab-data.csv ...\n",
            "dataset's shape: (5292, 12)\n",
            "adding 5292 entries to the original dataset...\n",
            "\n",
            "processing 2019-plu-total-hab-data.csv ...\n",
            "dataset's shape: (5616, 12)\n",
            "adding 5616 entries to the original dataset...\n",
            "\n",
            "processing 2020-plu-total-hab-data.csv ...\n",
            "dataset's shape: (2160, 12)\n",
            "adding 2160 entries to the original dataset...\n",
            "\n",
            "final merged dataset's shape: (30021, 12)\n",
            "number of entries in the merged dataset should be: 30021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAfC_HPTsyIK"
      },
      "source": [
        "## Save the updated (merged) dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EidA0aL4syIM"
      },
      "source": [
        "# Save with index as it contains the dates.\n",
        "cat_df.to_csv('avocado-updated-2020.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}